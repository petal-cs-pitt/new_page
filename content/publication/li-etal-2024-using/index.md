---
title: Using Large Language Models to Assess Young Students′ Writing Revisions
authors:
- Tianwen Li
- Zhexiong Liu
- Lindsay Matsumura
- Elaine Wang
- Diane Litman
- Richard Correnti
date: '2024-06-01'
publishDate: '2024-09-06T21:01:31.994918Z'
publication_types:
- '1'
publication: '*Proceedings of the 19th Workshop on Innovative Use of NLP for Building
  Educational Applications (BEA 2024)*'
abstract: Although effective revision is the crucial component of writing instruction,
  few automated writing evaluation (AWE) systems specifically focus on the quality
  of the revisions students undertake. In this study we investigate the use of a large
  language model (GPT-4) with Chain-of-Thought (CoT) prompting for assessing the quality
  of young students′ essay revisions aligned with the automated feedback messages
  they received. Results indicate that GPT-4 has significant potential for evaluating
  revision quality, particularly when detailed rubrics are included that describe
  common revision patterns shown by young writers. However, the addition of CoT prompting
  did not significantly improve performance. Further examination of GPT-4′s scoring
  performance across various levels of student writing proficiency revealed variable
  agreement with human ratings. The implications for improving AWE systems focusing
  on young students are discussed.
---

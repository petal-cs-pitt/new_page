---
title: 'Adding Argumentation into Human Evaluation of Long Document Abstractive Summarization:
  A Case Study on Legal Opinions'
authors:
- Mohamed Elaraby
- Huihui Xu
- Morgan Gray
- Kevin Ashley
- Diane Litman
date: '2024-05-01'
publishDate: '2024-09-06T21:02:58.728606Z'
publication_types:
- '1'
publication: '*Proceedings of the Fourth Workshop on Human Evaluation of NLP Systems
  (HumEval) @ LREC-COLING 2024*'
abstract: Human evaluation remains the gold standard for assessing abstractive summarization.
  However, current practices often prioritize constructing evaluation guidelines for
  fluency, coherence, and factual accuracy, overlooking other critical dimensions.
  In this paper, we investigate argument coverage in abstractive summarization by
  focusing on long legal opinions, where summaries must effectively encapsulate the
  documentâ€²s argumentative nature. We introduce a set of human-evaluation guidelines
  to evaluate generated summaries based on argumentative coverage. These guidelines
  enable us to assess three distinct summarization models, studying the influence
  of including argument roles in summarization. Furthermore, we utilize these evaluation
  scores to benchmark automatic summarization metrics against argument coverage, providing
  insights into the effectiveness of automated evaluation methods.

---

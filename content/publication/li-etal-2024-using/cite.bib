@inproceedings{li-etal-2024-using,
 abstract = {Although effective revision is the crucial component of writing instruction, few automated writing evaluation (AWE) systems specifically focus on the quality of the revisions students undertake. In this study we investigate the use of a large language model (GPT-4) with Chain-of-Thought (CoT) prompting for assessing the quality of young students′ essay revisions aligned with the automated feedback messages they received. Results indicate that GPT-4 has significant potential for evaluating revision quality, particularly when detailed rubrics are included that describe common revision patterns shown by young writers. However, the addition of CoT prompting did not significantly improve performance. Further examination of GPT-4′s scoring performance across various levels of student writing proficiency revealed variable agreement with human ratings. The implications for improving AWE systems focusing on young students are discussed.},
 address = {Mexico City, Mexico},
 author = {Li, Tianwen  and
Liu, Zhexiong  and
Matsumura, Lindsay  and
Wang, Elaine  and
Litman, Diane  and
Correnti, Richard},
 booktitle = {Proceedings of the 19th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2024)},
 editor = {Kochmar, Ekaterina  and
Bexte, Marie  and
Burstein, Jill  and
Horbach, Andrea  and
Laarmann-Quante, Ronja  and
Tack, Anaı̈s  and
Yaneva, Victoria  and
Yuan, Zheng},
 month = {June},
 pages = {365--380},
 publisher = {Association for Computational Linguistics},
 title = {Using Large Language Models to Assess Young Students′ Writing Revisions},
 url = {https://aclanthology.org/2024.bea-1.30},
 year = {2024}
}
